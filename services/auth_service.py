"""
è®¤è¯æœåŠ¡
Authentication and authorization services including HMAC validation and IP whitelist
"""
import json
import hashlib
import time
import logging
from typing import Tuple, Optional, Dict, Any

from services.redis_service import redis_service
from models.config import config
from utils.helpers import validate_token, extract_match_key
from utils.cidr_matcher import CIDRMatcher
from utils.browser_detector import BrowserDetector

logger = logging.getLogger(__name__)


def is_ip_in_fixed_whitelist(client_ip: str) -> bool:
    """
    æ£€æŸ¥IPæ˜¯å¦åœ¨å›ºå®šç™½åå•ä¸­
    æ”¯æŒå•ä¸ªIPå’ŒCIDRæ ¼å¼
    
    Args:
        client_ip: å®¢æˆ·ç«¯IPåœ°å€
    
    Returns:
        bool: å¦‚æœIPåœ¨å›ºå®šç™½åå•ä¸­è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not config.FIXED_IP_WHITELIST:
        return False
    
    try:
        # ä½¿ç”¨CIDRåŒ¹é…å™¨æ£€æŸ¥IPæ˜¯å¦åŒ¹é…ç™½åå•ä¸­çš„ä»»ä½•æ¨¡å¼
        is_match, matched_pattern = CIDRMatcher.match_ip_against_patterns(
            client_ip, 
            config.FIXED_IP_WHITELIST
        )
        if is_match:
            logger.info(f"âœ… å›ºå®šç™½åå•éªŒè¯æˆåŠŸ: IP={client_ip} åŒ¹é…æ¨¡å¼={matched_pattern}")
            return True
        return False
    except Exception as e:
        logger.error(f"æ£€æŸ¥å›ºå®šç™½åå•å¤±è´¥: IP={client_ip}, error={str(e)}")
        return False


async def check_ip_key_path(client_ip: str, path: str, user_agent: str) -> Tuple[bool, Optional[str]]:
    """
    æ£€æŸ¥IPæ˜¯å¦æœ‰æƒè®¿é—®æŒ‡å®šè·¯å¾„ï¼ˆä½¿ç”¨CIDRç™½åå•ï¼‰
    
    Returns:
        (is_allowed, whitelist_uid)
    """
    # é¦–å…ˆæ£€æŸ¥å›ºå®šç™½åå• - å¦‚æœåœ¨å›ºå®šç™½åå•ä¸­ï¼Œç›´æ¥æ”¾è¡Œ
    if is_ip_in_fixed_whitelist(client_ip):
        logger.info(f"ğŸ”“ å›ºå®šç™½åå•æ”¾è¡Œ: IP={client_ip}, path={path}")
        return True, "fixed_whitelist"
    
    redis_client = redis_service.get_client()
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé™æ€æ–‡ä»¶ä¸”å¯ç”¨äº†IP-onlyéªŒè¯
        is_static_file = path.lower().endswith(config.STATIC_FILE_EXTENSIONS)
        skip_path_check = is_static_file and config.ENABLE_STATIC_FILE_IP_ONLY_CHECK
        
        if skip_path_check:
            logger.info(f"é™æ€æ–‡ä»¶IP-onlyéªŒè¯æ¨¡å¼: path={path}")
            
            # é¦–å…ˆæ£€æŸ¥ç‹¬ç«‹çš„é™æ€æ–‡ä»¶ç™½åå•
            is_static_allowed, static_uid = await check_static_file_access(client_ip, user_agent)
            if is_static_allowed:
                logger.info(f"âœ… é™æ€æ–‡ä»¶ç‹¬ç«‹ç™½åå•éªŒè¯æˆåŠŸ: IP={client_ip}, uid={static_uid}, path={path}")
                return True, static_uid
        
        requested_key_path = extract_match_key(path)
        if not requested_key_path and not skip_path_check:
            logger.debug(f"æ— æ•ˆçš„ key_path: path={path}")
            return False, None
        
        ua_hash = hashlib.md5(user_agent.encode()).hexdigest()[:8]
        
        # ç»Ÿä¸€CIDRåŒ¹é…æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„CIDRæ¨¡å¼
        cidr_pattern = f"ip_cidr_access:*:{ua_hash}"
        cidr_keys = await redis_client.keys(cidr_pattern)
        
        stored_key_path = None
        stored_uid = None
        
        for cidr_key in cidr_keys:
            cidr_data = await redis_client.get(cidr_key)
            if cidr_data:
                try:
                    data = json.loads(cidr_data)
                    ip_patterns = data.get("ip_patterns", [])
                    
                    # ä½¿ç”¨CIDRåŒ¹é…æ£€æŸ¥IP
                    is_match, matched_pattern = CIDRMatcher.match_ip_against_patterns(client_ip, ip_patterns)
                    if is_match:
                        stored_uid = data.get("uid")
                        
                        # å¯¹äºé™æ€æ–‡ä»¶ä¸”å¯ç”¨IP-onlyæ£€æŸ¥ï¼ŒåªéªŒè¯IP+UAï¼Œè·³è¿‡è·¯å¾„æ£€æŸ¥
                        if skip_path_check:
                            logger.info(f"âœ… é™æ€æ–‡ä»¶IP+UAéªŒè¯æˆåŠŸï¼ˆè·¯å¾„ç™½åå•ï¼‰: IP={client_ip} åŒ¹é…æ¨¡å¼={matched_pattern}, uid={stored_uid}, path={path}")
                            return True, stored_uid
                        
                        # æ­£å¸¸æ¨¡å¼ï¼šæ£€æŸ¥å¤šè·¯å¾„æ”¯æŒ
                        paths = data.get("paths", [])
                        if paths:
                            # æ£€æŸ¥è¯·æ±‚çš„è·¯å¾„æ˜¯å¦åœ¨å­˜å‚¨çš„è·¯å¾„åˆ—è¡¨ä¸­
                            for path_info in paths:
                                stored_path = path_info.get("key_path")
                                if stored_path and stored_path == requested_key_path:
                                    stored_key_path = stored_path
                                    logger.info(f"âœ… CIDRåŒ¹é…æˆåŠŸ: IP={client_ip} åŒ¹é…æ¨¡å¼={matched_pattern}, è·¯å¾„={stored_path}")
                                    break
                        else:
                            # å‘åå…¼å®¹ï¼šä½¿ç”¨å•ä¸€key_path
                            if data.get("key_path", "") == requested_key_path:
                                stored_key_path = data.get("key_path")
                                logger.info(f"âœ… CIDRåŒ¹é…æˆåŠŸ: IP={client_ip} åŒ¹é…æ¨¡å¼={matched_pattern}, è·¯å¾„={stored_key_path}")
                        if stored_key_path:
                            break
                except json.JSONDecodeError:
                    continue
        
        if not stored_key_path and not (skip_path_check and stored_uid):
            # åˆ¤æ–­æ˜¯å¦ä¸ºé™æ€æ–‡ä»¶ï¼ˆå¯èƒ½ç”±JSç™½åå•éªŒè¯ï¼‰
            is_potential_js_whitelist = (
                is_static_file or 
                path.lower().endswith(('.m3u8', '.ts', 'enc.key', '.jpg', '.png', '.gif', '.svg', '.ico'))
            )
            
            # å¦‚æœæ˜¯é™æ€æ–‡ä»¶ä¸”å¯ç”¨äº†JSç™½åå•ï¼Œä½¿ç”¨DEBUGçº§åˆ«ï¼ˆé¿å…å™ªéŸ³ï¼‰
            # å› ä¸ºåç»­å¯èƒ½é€šè¿‡JSç™½åå•éªŒè¯
            if is_potential_js_whitelist and config.ENABLE_JS_WHITELIST_TRACKER:
                logger.debug(
                    f"åç«¯IPéªŒè¯æœªé€šè¿‡ï¼ˆå°†å°è¯•JSç™½åå•ï¼‰: IP={client_ip}, "
                    f"UA hash={ua_hash}, requested_key={requested_key_path}"
                )
            else:
                logger.warning(
                    f"âŒ IPè®¿é—®è¢«æ‹’ç»: IP={client_ip} æœªæ‰¾åˆ°åŒ¹é…çš„CIDRæ¨¡å¼, "
                    f"UA hash={ua_hash}, requested_key={requested_key_path}"
                )
            return False, None
        
        # å¦‚æœæ˜¯é™æ€æ–‡ä»¶IP-onlyæ¨¡å¼ï¼Œå‰é¢å·²ç»è¿”å›äº†
        # è¿™é‡Œå¤„ç†æ­£å¸¸æ¨¡å¼çš„è·¯å¾„æ£€æŸ¥
        if stored_key_path:
            # æ£€æŸ¥è®¿é—®æƒé™ï¼ˆä½¿ç”¨åŸå§‹çš„substringåŒ¹é…é€»è¾‘ï¼‰
            if stored_key_path.lower() not in path.lower():
                logger.warning(f"âŒ è®¿é—®è¢«æ‹’ç»: IP={client_ip}, path={path}, requested_key={requested_key_path}, allowed_key={stored_key_path}")
                return False, stored_uid
            
            logger.info(f"âœ… è®¿é—®å…è®¸: IP={client_ip}, path={path}, key_path={stored_key_path}, uid={stored_uid}")
            return True, stored_uid
        
        return False, None
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥ key_path å¤±è´¥: IP={client_ip}, path={path}, error={str(e)}")
        return False, None


async def check_m3u8_access_count_adaptive(
    uid: str,
    full_url: str,
    client_ip: str,
    user_agent: str
) -> Tuple[bool, Dict[str, Any]]:
    """
    åŸºäºæµè§ˆå™¨ç±»å‹çš„è‡ªé€‚åº” M3U8 è®¿é—®æ¬¡æ•°æ£€æŸ¥
    
    Returns:
        (is_allowed, access_info)
    """
    redis_client = redis_service.get_client()
    
    try:
        # æ£€æµ‹æµè§ˆå™¨ç±»å‹
        browser_type, browser_name, suggested_max_count = BrowserDetector.detect_browser_type(user_agent)
        
        # è·å–é…ç½®çš„è®¿é—®æ¬¡æ•°é™åˆ¶
        if config.ENABLE_BROWSER_ADAPTIVE_ACCESS:
            access_limits = config.M3U8_ACCESS_LIMITS.get(browser_type, {})
            max_access_count = access_limits.get(browser_name, access_limits.get('default', suggested_max_count))
            access_window_ttl = config.M3U8_ACCESS_WINDOW_TTL.get(browser_type, 60)
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸå§‹é…ç½®
            max_access_count = config.M3U8_DEFAULT_MAX_ACCESS_COUNT
            access_window_ttl = config.M3U8_SINGLE_USE_TTL if hasattr(config, 'M3U8_SINGLE_USE_TTL') else 60
        
        # ç”Ÿæˆè¯·æ±‚æ ‡è¯†
        request_identifier = f"{uid}:{full_url}:{client_ip}"
        request_hash = hashlib.sha256(request_identifier.encode()).hexdigest()
        redis_key = f"m3u8_access_count_v2:{request_hash}"
        
        logger.info(f"M3U8è®¿é—®æ£€æŸ¥: uid={uid}, browser_type={browser_type}, browser_name={browser_name}, "
                   f"max_count={max_access_count}, window_ttl={access_window_ttl}s")
        
        # ä½¿ç”¨RedisåŸå­æ“ä½œé€’å¢è®¡æ•°å™¨
        current_count = await redis_client.incr(redis_key)
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è®¿é—®ï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´
        if current_count == 1:
            await redis_client.expire(redis_key, access_window_ttl)
            logger.info(f"M3U8é¦–æ¬¡è®¿é—®: uid={uid}, browser={browser_name}")
            
            access_info = {
                "browser_type": browser_type,
                "browser_name": browser_name,
                "current_count": current_count,
                "max_count": max_access_count,
                "window_ttl": access_window_ttl,
                "remaining_count": max_access_count - current_count,
                "is_first_access": True
            }
            return True, access_info
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è®¿é—®æ¬¡æ•°
        if current_count <= max_access_count:
            remaining_ttl = await redis_client.ttl(redis_key)
            logger.info(f"M3U8è®¿é—®å…è®¸: uid={uid}, browser={browser_name}, count={current_count}/{max_access_count}")
            
            access_info = {
                "browser_type": browser_type,
                "browser_name": browser_name,
                "current_count": current_count,
                "max_count": max_access_count,
                "remaining_ttl": remaining_ttl,
                "remaining_count": max_access_count - current_count,
                "is_first_access": False
            }
            return True, access_info
        else:
            remaining_ttl = await redis_client.ttl(redis_key)
            logger.warning(f"M3U8è®¿é—®æ¬¡æ•°è¶…é™: uid={uid}, browser={browser_name}, count={current_count}/{max_access_count}")
            
            access_info = {
                "browser_type": browser_type,
                "browser_name": browser_name,
                "current_count": current_count,
                "max_count": max_access_count,
                "remaining_ttl": remaining_ttl,
                "remaining_count": 0,
                "is_first_access": False,
                "exceeded": True
            }
            return False, access_info
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥M3U8è®¿é—®æ¬¡æ•°å¤±è´¥: {str(e)}")
        access_info = {
            "browser_type": "unknown",
            "browser_name": "unknown",
            "error": str(e)
        }
        return False, access_info


async def check_m3u8_access_count(uid: str, full_url: str, client_ip: str, user_agent: str) -> bool:
    """å‘åå…¼å®¹çš„ M3U8 è®¿é—®æ£€æŸ¥å‡½æ•°"""
    is_allowed, _ = await check_m3u8_access_count_adaptive(uid, full_url, client_ip, user_agent)
    return is_allowed


async def add_ip_to_whitelist(
    uid: str,
    path: str,
    target_client_ip: str,
    user_agent: str
) -> Dict[str, Any]:
    """
    æ·»åŠ IPåˆ°ç™½åå•
    
    Returns:
        Result dictionary with success/error information
    """
    redis_client = redis_service.get_client()
    
    try:
        # Extract key path
        key_path = extract_match_key(path)
        if not key_path:
            return {
                "success": False,
                "error": "Invalid path format"
            }
        
        # æ ‡å‡†åŒ–IPåœ°å€ï¼ˆè‡ªåŠ¨è½¬æ¢ä¸º/24å­ç½‘ï¼‰
        if CIDRMatcher.is_valid_ip(target_client_ip) or CIDRMatcher.is_cidr_notation(target_client_ip):
            normalized_pattern = CIDRMatcher.normalize_cidr(target_client_ip)
            logger.info(f"å·²æ ‡å‡†åŒ–IPæ¨¡å¼: {target_client_ip} -> {normalized_pattern}")
        else:
            return {
                "success": False,
                "error": f"Invalid IP address or CIDR: {target_client_ip}"
            }
        
        # Store in Redis using unified CIDR approach
        ua_hash = hashlib.md5(user_agent.encode()).hexdigest()[:8]
        current_time = int(time.time())
        
        # ç»Ÿä¸€ä½¿ç”¨CIDRé”®æ ¼å¼å­˜å‚¨æ‰€æœ‰IP
        redis_key = f"ip_cidr_access:{normalized_pattern.replace('/', '_')}:{ua_hash}"
        
        # æ„å»ºæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šè·¯å¾„å­˜å‚¨
        whitelist_data = {
            "uid": uid,
            "key_path": key_path,
            "paths": [{"key_path": key_path, "created_at": current_time}],
            "ip_patterns": [normalized_pattern],
            "user_agent": user_agent,
            "created_at": current_time
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆå¹¶è·¯å¾„
        existing_data_str = await redis_client.get(redis_key)
        merged_count = 0
        new_count = 1
        
        if existing_data_str:
            try:
                existing_data = json.loads(existing_data_str)
                existing_paths = existing_data.get("paths", [])
                
                # æ£€æŸ¥æ–°è·¯å¾„æ˜¯å¦å·²å­˜åœ¨
                path_exists = any(p.get("key_path") == key_path for p in existing_paths)
                
                if not path_exists:
                    # æ·»åŠ æ–°è·¯å¾„
                    existing_paths.append({"key_path": key_path, "created_at": current_time})
                    
                    # ä¿æŒæœ€å¤šé…ç½®çš„è·¯å¾„æ•°
                    if len(existing_paths) > config.MAX_PATHS_PER_CIDR:
                        existing_paths.sort(key=lambda x: x.get("created_at", 0))
                        # è·å–è¢«ç§»é™¤çš„æ—§è·¯å¾„
                        removed_paths = existing_paths[:-config.MAX_PATHS_PER_CIDR]
                        existing_paths = existing_paths[-config.MAX_PATHS_PER_CIDR:]
                        
                        # æ¸…ç†æ—§è·¯å¾„çš„m3u8è®¿é—®è®¡æ•°å™¨
                        for old_path in removed_paths:
                            old_key_path = old_path.get("key_path")
                            if old_key_path:
                                # æ¸…ç†å¯èƒ½çš„m3u8è®¿é—®è®¡æ•°å™¨
                                pattern = f"m3u8_access_count_v2:*{old_key_path}*"
                                old_keys = await redis_client.keys(pattern)
                                if old_keys:
                                    await redis_client.delete(*old_keys)
                                    logger.info(f"æ¸…ç†æ—§è·¯å¾„ {old_key_path} çš„ {len(old_keys)} ä¸ªè®¿é—®è®¡æ•°å™¨")
                    
                    existing_data["paths"] = existing_paths
                    existing_data["key_path"] = key_path
                    logger.info(f"ä¸ºCIDRæ¨¡å¼ {normalized_pattern} æ·»åŠ æ–°è·¯å¾„: {key_path}, æ€»è·¯å¾„æ•°: {len(existing_paths)}")
                else:
                    # è·¯å¾„å·²å­˜åœ¨ï¼Œæ›´æ–°æ—¶é—´æˆ³
                    for p in existing_paths:
                        if p.get("key_path") == key_path:
                            p["created_at"] = current_time
                            break
                    existing_data["paths"] = existing_paths
                    logger.info(f"æ›´æ–°CIDRæ¨¡å¼ {normalized_pattern} ç°æœ‰è·¯å¾„æ—¶é—´æˆ³: {key_path}")
                
                whitelist_data = existing_data
                merged_count = 1
                new_count = 0
            except json.JSONDecodeError:
                pass
        
        # UIDçº§åˆ«UA+IPå¯¹ç®¡ç†ï¼šè¿½è¸ªæ‰€æœ‰UA+IPç»„åˆ
        uid_pairs_key = f"uid_ua_ip_pairs:{uid}"
        ua_ip_pair_id = f"{normalized_pattern}:{ua_hash}"
        
        # è·å–å½“å‰UIDçš„æ‰€æœ‰UA+IPå¯¹
        uid_pairs_data_str = await redis_client.get(uid_pairs_key)
        uid_pairs = []
        removed_pairs = []
        
        if uid_pairs_data_str:
            try:
                uid_pairs = json.loads(uid_pairs_data_str)
            except json.JSONDecodeError:
                uid_pairs = []
        
        # æ£€æŸ¥å½“å‰UA+IPå¯¹æ˜¯å¦å·²å­˜åœ¨
        existing_pair = None
        for pair in uid_pairs:
            if pair.get("pair_id") == ua_ip_pair_id:
                existing_pair = pair
                break
        
        if existing_pair:
            # æ›´æ–°ç°æœ‰å¯¹çš„æ—¶é—´æˆ³
            existing_pair["last_updated"] = current_time
        else:
            # æ·»åŠ æ–°çš„UA+IPå¯¹
            new_pair = {
                "pair_id": ua_ip_pair_id,
                "ip_pattern": normalized_pattern,
                "ua_hash": ua_hash,
                "created_at": current_time,
                "last_updated": current_time
            }
            uid_pairs.append(new_pair)
            
            # å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œç§»é™¤æœ€æ—§çš„ï¼ˆFIFOï¼‰
            if len(uid_pairs) > config.MAX_UA_IP_PAIRS_PER_UID:
                uid_pairs.sort(key=lambda x: x.get("created_at", 0))
                removed_pairs = uid_pairs[:-config.MAX_UA_IP_PAIRS_PER_UID]
                uid_pairs = uid_pairs[-config.MAX_UA_IP_PAIRS_PER_UID:]
                
                # æ¸…ç†è¢«ç§»é™¤çš„UA+IPå¯¹çš„Redisé”®
                for old_pair in removed_pairs:
                    old_pair_id = old_pair.get("pair_id", "")
                    if old_pair_id and ":" in old_pair_id:
                        try:
                            old_ip_pattern, old_ua_hash = old_pair_id.rsplit(":", 1)
                            old_redis_key = f"ip_cidr_access:{old_ip_pattern.replace('/', '_')}:{old_ua_hash}"
                            await redis_client.delete(old_redis_key)
                            logger.info(f"æ¸…ç†æ—§UA+IPå¯¹: uid={uid}, pair_id={old_pair_id}")
                        except ValueError as e:
                            logger.error(f"æ¸…ç†æ—§UA+IPå¯¹å¤±è´¥ï¼Œpair_idæ ¼å¼æ— æ•ˆ: {old_pair_id}, error={str(e)}")
        
        # å­˜å‚¨æ›´æ–°çš„UIDçº§åˆ«UA+IPå¯¹åˆ—è¡¨
        await redis_client.set(uid_pairs_key, json.dumps(uid_pairs), ex=config.IP_ACCESS_TTL)
        
        # å­˜å‚¨æ›´æ–°çš„IP+UAæ•°æ®
        await redis_client.set(redis_key, json.dumps(whitelist_data), ex=config.IP_ACCESS_TTL)
        
        # ç”ŸæˆCIDRç¤ºä¾‹ç”¨äºè°ƒè¯•
        cidr_examples = CIDRMatcher.expand_cidr_examples(normalized_pattern, 3)
        
        logger.info(f"å­˜å‚¨IPæ¨¡å¼æˆåŠŸ: patterns=[{normalized_pattern}], ua_hash={ua_hash}, TTL={config.IP_ACCESS_TTL}s")
        logger.info(f"UID UA+IPå¯¹ç®¡ç†: uid={uid}, total_pairs={len(uid_pairs)}, removed_pairs={len(removed_pairs)}")
        
        return {
            "success": True,
            "message": "CIDR whitelist added/updated successfully",
            "key_path": key_path,
            "ip_pattern": normalized_pattern,
            "cidr_examples": cidr_examples,
            "ua_hash": ua_hash,
            "ttl": config.IP_ACCESS_TTL,
            "patterns_merged": merged_count,
            "patterns_new": new_count,
            "multi_path_info": {
                "max_paths_per_cidr": config.MAX_PATHS_PER_CIDR,
                "current_path": key_path,
                "path_replacement_policy": "FIFO (oldest paths are removed when limit exceeded)"
            },
            "uid_ua_ip_pairs_info": {
                "max_pairs_per_uid": config.MAX_UA_IP_PAIRS_PER_UID,
                "current_pairs_count": len(uid_pairs),
                "pairs_removed": len(removed_pairs),
                "pair_replacement_policy": "FIFO (oldest UA+IP pairs are removed when limit exceeded)"
            }
        }
        
    except Exception as e:
        logger.error(f"add_ip_whitelist error: {str(e)}")
        return {
            "success": False,
            "error": f"Failed to add IP to whitelist: {str(e)}"
        }


async def add_static_file_whitelist(
    uid: str,
    target_client_ip: str,
    user_agent: str
) -> Dict[str, Any]:
    """
    æ·»åŠ UA+IPåˆ°é™æ€æ–‡ä»¶ç™½åå•ï¼ˆç‹¬ç«‹å­˜å‚¨ï¼Œæ— éœ€è·¯å¾„ï¼‰
    
    Args:
        uid: ç”¨æˆ·ID
        target_client_ip: ç›®æ ‡å®¢æˆ·ç«¯IP
        user_agent: User-Agent
    
    Returns:
        Result dictionary with success/error information
    """
    redis_client = redis_service.get_client()
    
    try:
        # æ ‡å‡†åŒ–IPåœ°å€ï¼ˆè‡ªåŠ¨è½¬æ¢ä¸º/24å­ç½‘ï¼‰
        if CIDRMatcher.is_valid_ip(target_client_ip) or CIDRMatcher.is_cidr_notation(target_client_ip):
            normalized_pattern = CIDRMatcher.normalize_cidr(target_client_ip)
            logger.info(f"é™æ€æ–‡ä»¶ç™½åå• - å·²æ ‡å‡†åŒ–IPæ¨¡å¼: {target_client_ip} -> {normalized_pattern}")
        else:
            return {
                "success": False,
                "error": f"Invalid IP address or CIDR: {target_client_ip}"
            }
        
        ua_hash = hashlib.md5(user_agent.encode()).hexdigest()[:8]
        current_time = int(time.time())
        
        # ä½¿ç”¨ç‹¬ç«‹çš„Redisé”®æ ¼å¼å­˜å‚¨é™æ€æ–‡ä»¶ç™½åå•
        redis_key = f"static_file_access:{normalized_pattern.replace('/', '_')}:{ua_hash}"
        
        # æ„å»ºæ•°æ®ç»“æ„
        whitelist_data = {
            "uid": uid,
            "ip_patterns": [normalized_pattern],
            "user_agent": user_agent,
            "created_at": current_time,
            "access_type": "static_files_only"
        }
        
        # UIDçº§åˆ«UA+IPå¯¹ç®¡ç†ï¼šè¿½è¸ªæ‰€æœ‰UA+IPç»„åˆ
        uid_pairs_key = f"uid_static_ua_ip_pairs:{uid}"
        ua_ip_pair_id = f"{normalized_pattern}:{ua_hash}"
        
        # è·å–å½“å‰UIDçš„æ‰€æœ‰UA+IPå¯¹
        uid_pairs_data_str = await redis_client.get(uid_pairs_key)
        uid_pairs = []
        removed_pairs = []
        
        if uid_pairs_data_str:
            try:
                uid_pairs = json.loads(uid_pairs_data_str)
            except json.JSONDecodeError:
                uid_pairs = []
        
        # æ£€æŸ¥å½“å‰UA+IPå¯¹æ˜¯å¦å·²å­˜åœ¨
        existing_pair = None
        for pair in uid_pairs:
            if pair.get("pair_id") == ua_ip_pair_id:
                existing_pair = pair
                break
        
        if existing_pair:
            # æ›´æ–°ç°æœ‰å¯¹çš„æ—¶é—´æˆ³
            existing_pair["last_updated"] = current_time
        else:
            # æ·»åŠ æ–°çš„UA+IPå¯¹
            new_pair = {
                "pair_id": ua_ip_pair_id,
                "ip_pattern": normalized_pattern,
                "ua_hash": ua_hash,
                "created_at": current_time,
                "last_updated": current_time
            }
            uid_pairs.append(new_pair)
            
            # å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œç§»é™¤æœ€æ—§çš„ï¼ˆFIFOï¼‰
            if len(uid_pairs) > config.MAX_UA_IP_PAIRS_PER_UID:
                uid_pairs.sort(key=lambda x: x.get("created_at", 0))
                removed_pairs = uid_pairs[:-config.MAX_UA_IP_PAIRS_PER_UID]
                uid_pairs = uid_pairs[-config.MAX_UA_IP_PAIRS_PER_UID:]
                
                # æ¸…ç†è¢«ç§»é™¤çš„UA+IPå¯¹çš„Redisé”®
                for old_pair in removed_pairs:
                    old_pair_id = old_pair.get("pair_id", "")
                    if old_pair_id and ":" in old_pair_id:
                        try:
                            old_ip_pattern, old_ua_hash = old_pair_id.rsplit(":", 1)
                            old_redis_key = f"static_file_access:{old_ip_pattern.replace('/', '_')}:{old_ua_hash}"
                            await redis_client.delete(old_redis_key)
                            logger.info(f"æ¸…ç†æ—§é™æ€æ–‡ä»¶UA+IPå¯¹: uid={uid}, pair_id={old_pair_id}")
                        except ValueError as e:
                            logger.error(f"æ¸…ç†æ—§é™æ€æ–‡ä»¶UA+IPå¯¹å¤±è´¥ï¼Œpair_idæ ¼å¼æ— æ•ˆ: {old_pair_id}, error={str(e)}")
        
        # å­˜å‚¨æ›´æ–°çš„UIDçº§åˆ«UA+IPå¯¹åˆ—è¡¨
        await redis_client.set(uid_pairs_key, json.dumps(uid_pairs), ex=config.IP_ACCESS_TTL)
        
        # å­˜å‚¨é™æ€æ–‡ä»¶ç™½åå•æ•°æ®
        await redis_client.set(redis_key, json.dumps(whitelist_data), ex=config.IP_ACCESS_TTL)
        
        # ç”ŸæˆCIDRç¤ºä¾‹ç”¨äºè°ƒè¯•
        cidr_examples = CIDRMatcher.expand_cidr_examples(normalized_pattern, 3)
        
        logger.info(f"å­˜å‚¨é™æ€æ–‡ä»¶ç™½åå•æˆåŠŸ: patterns=[{normalized_pattern}], ua_hash={ua_hash}, TTL={config.IP_ACCESS_TTL}s")
        logger.info(f"UID é™æ€æ–‡ä»¶UA+IPå¯¹ç®¡ç†: uid={uid}, total_pairs={len(uid_pairs)}, removed_pairs={len(removed_pairs)}")
        
        return {
            "success": True,
            "message": "Static file whitelist added/updated successfully",
            "ip_pattern": normalized_pattern,
            "cidr_examples": cidr_examples,
            "ua_hash": ua_hash,
            "ttl": config.IP_ACCESS_TTL,
            "uid_static_ua_ip_pairs_info": {
                "max_pairs_per_uid": config.MAX_UA_IP_PAIRS_PER_UID,
                "current_pairs_count": len(uid_pairs),
                "pairs_removed": len(removed_pairs),
                "pair_replacement_policy": "FIFO (oldest UA+IP pairs are removed when limit exceeded)"
            }
        }
        
    except Exception as e:
        logger.error(f"add_static_file_whitelist error: {str(e)}")
        return {
            "success": False,
            "error": f"Failed to add static file whitelist: {str(e)}"
        }


async def check_static_file_access(client_ip: str, user_agent: str) -> Tuple[bool, Optional[str]]:
    """
    æ£€æŸ¥IP+UAæ˜¯å¦æœ‰æƒè®¿é—®é™æ€æ–‡ä»¶ï¼ˆç‹¬ç«‹ç™½åå•ï¼‰
    
    Args:
        client_ip: å®¢æˆ·ç«¯IP
        user_agent: User-Agent
    
    Returns:
        (is_allowed, uid)
    """
    redis_client = redis_service.get_client()
    try:
        ua_hash = hashlib.md5(user_agent.encode()).hexdigest()[:8]
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„é™æ€æ–‡ä»¶è®¿é—®é”®
        pattern = f"static_file_access:*:{ua_hash}"
        static_keys = await redis_client.keys(pattern)
        
        for static_key in static_keys:
            static_data = await redis_client.get(static_key)
            if static_data:
                try:
                    data = json.loads(static_data)
                    ip_patterns = data.get("ip_patterns", [])
                    
                    # ä½¿ç”¨CIDRåŒ¹é…æ£€æŸ¥IP
                    is_match, matched_pattern = CIDRMatcher.match_ip_against_patterns(client_ip, ip_patterns)
                    if is_match:
                        uid = data.get("uid")
                        logger.info(f"âœ… é™æ€æ–‡ä»¶ç™½åå•éªŒè¯æˆåŠŸ: IP={client_ip} åŒ¹é…æ¨¡å¼={matched_pattern}, uid={uid}")
                        return True, uid
                except json.JSONDecodeError:
                    continue
        
        logger.debug(f"é™æ€æ–‡ä»¶ç™½åå•æœªæ‰¾åˆ°åŒ¹é…: IP={client_ip}, UA hash={ua_hash}")
        return False, None
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥é™æ€æ–‡ä»¶è®¿é—®å¤±è´¥: IP={client_ip}, error={str(e)}")
        return False, None
