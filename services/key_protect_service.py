"""
Key æ–‡ä»¶åŠ¨æ€ä¿æŠ¤æœåŠ¡
é€šè¿‡åŠ¨æ€ä¿®æ”¹ m3u8 æ–‡ä»¶ä¸­çš„ key URI å®ç° .key æ–‡ä»¶è®¿é—®æ§åˆ¶ï¼Œé˜²æ­¢é‡æ”¾æ”»å‡»

åŠŸèƒ½è¯´æ˜ï¼š
- å½“ç”¨æˆ·è¯·æ±‚ m3u8 æ–‡ä»¶æ—¶ï¼ŒåŠ¨æ€ä¿®æ”¹æ–‡ä»¶å†…å®¹
- åœ¨ #EXT-X-KEY æ ‡ç­¾çš„ URI ä¸­æ·»åŠ  uidã€tokenã€expires å‚æ•°
- è¿™æ ·æ’­æ”¾å™¨è¯·æ±‚ .key æ–‡ä»¶æ—¶ä¼šè‡ªåŠ¨å¸¦ä¸Šè¿™äº›å‚æ•°
- ä½¿ç”¨ç°æœ‰çš„ token éªŒè¯æœºåˆ¶è¿›è¡Œè®¿é—®æ§åˆ¶

å·¥ä½œåŸç†ï¼š
1. ç”¨æˆ·è¯·æ±‚ m3u8 æ–‡ä»¶ï¼š/video/index.m3u8?uid=315&expires=xxx&token=xxx
2. ç³»ç»Ÿè¯»å–åŸå§‹ m3u8 å†…å®¹
3. æ‰¾åˆ° #EXT-X-KEY:METHOD=AES-128,URI="enc.key" 
4. åŠ¨æ€ä¿®æ”¹ä¸º #EXT-X-KEY:METHOD=AES-128,URI="enc.key?uid=315&expires=xxx&token=xxx"
5. æ’­æ”¾å™¨è¯·æ±‚ enc.key?uid=315&expires=xxx&token=xxx
6. ç³»ç»Ÿä½¿ç”¨ç°æœ‰ token éªŒè¯æœºåˆ¶æ£€æŸ¥è®¿é—®æƒé™

é…ç½®é¡¹ï¼ˆåœ¨ models/config.py ä¸­ï¼‰ï¼š
- KEY_PROTECT_ENABLED: æ˜¯å¦å¯ç”¨ .key æ–‡ä»¶åŠ¨æ€ä¿æŠ¤
- KEY_PROTECT_DYNAMIC_M3U8: æ˜¯å¦åŠ¨æ€ä¿®æ”¹ m3u8 å†…å®¹
- KEY_PROTECT_MAX_USES: æ¯ä¸ª token å…³è”çš„ .key æ–‡ä»¶æœ€å¤§è®¿é—®æ¬¡æ•°
- KEY_PROTECT_TTL: key ä¿æŠ¤è®°å½•çš„ TTLï¼ˆç§’ï¼‰
- KEY_PROTECT_EXTENSIONS: éœ€è¦ä¿æŠ¤çš„å¯†é’¥æ–‡ä»¶æ‰©å±•åï¼ˆæ”¯æŒæ‰©å±•åå’Œæ–‡ä»¶åæ¨¡å¼ï¼‰
"""
import asyncio
import json
import logging
import hashlib
import hmac
import time
import re
import os
from typing import Tuple, Dict, Any, Optional, List
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse

from services.redis_service import redis_service

logger = logging.getLogger(__name__)

# Redis key prefixes
KEY_PROTECT_ACCESS_PREFIX = "key_protect:access:"  # å­˜å‚¨ key æ–‡ä»¶è®¿é—®è®¡æ•°
KEY_PROTECT_LOG_KEY = "key_protect:logs"  # å­˜å‚¨è®¿é—®æ—¥å¿—
M3U8_CONTENT_CACHE_PREFIX = "m3u8_content:"  # å­˜å‚¨ m3u8 åŸå§‹å†…å®¹ç¼“å­˜
MAX_LOG_RECORDS = 300

# Background task set to prevent garbage collection of fire-and-forget tasks
_background_tasks = set()


def _schedule_background_task(coro):
    """
    Schedule a coroutine as a fire-and-forget background task.
    Prevents garbage collection and handles exceptions gracefully.
    """
    def _task_done_callback(task):
        _background_tasks.discard(task)
        try:
            exc = task.exception()
            if exc:
                logger.error(f"Background task failed: {exc}")
        except asyncio.CancelledError:
            pass
        except asyncio.InvalidStateError:
            pass
    
    task = asyncio.create_task(coro)
    _background_tasks.add(task)
    task.add_done_callback(_task_done_callback)
    return task


async def get_cached_m3u8_content(path: str) -> Optional[str]:
    """
    ä» Redis è·å–ç¼“å­˜çš„ m3u8 åŸå§‹å†…å®¹
    
    Args:
        path: m3u8 æ–‡ä»¶è·¯å¾„
    
    Returns:
        ç¼“å­˜çš„ m3u8 å†…å®¹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
    """
    try:
        redis_client = redis_service.get_client()
        
        # ä½¿ç”¨è·¯å¾„çš„å“ˆå¸Œä½œä¸º key
        path_hash = hashlib.sha256(path.encode()).hexdigest()[:32]
        cache_key = f"{M3U8_CONTENT_CACHE_PREFIX}{path_hash}"
        
        cached_content = await redis_client.get(cache_key)
        
        if cached_content:
            logger.debug(f"ğŸ“¦ M3U8 ç¼“å­˜å‘½ä¸­: path={path}")
            # Redis è¿”å›çš„å¯èƒ½æ˜¯ bytes
            if isinstance(cached_content, bytes):
                return cached_content.decode('utf-8')
            return cached_content
        
        logger.debug(f"ğŸ“¦ M3U8 ç¼“å­˜æœªå‘½ä¸­: path={path}")
        return None
        
    except Exception as e:
        logger.error(f"è·å– m3u8 ç¼“å­˜å¤±è´¥: path={path}, error={str(e)}")
        return None


async def set_cached_m3u8_content(path: str, content: str, ttl: int) -> bool:
    """
    å°† m3u8 åŸå§‹å†…å®¹å­˜å…¥ Redis ç¼“å­˜
    
    Args:
        path: m3u8 æ–‡ä»¶è·¯å¾„
        content: m3u8 åŸå§‹å†…å®¹
        ttl: ç¼“å­˜ TTLï¼ˆç§’ï¼‰
    
    Returns:
        æ˜¯å¦æˆåŠŸç¼“å­˜
    """
    try:
        redis_client = redis_service.get_client()
        
        # ä½¿ç”¨è·¯å¾„çš„å“ˆå¸Œä½œä¸º key
        path_hash = hashlib.sha256(path.encode()).hexdigest()[:32]
        cache_key = f"{M3U8_CONTENT_CACHE_PREFIX}{path_hash}"
        
        await redis_client.setex(cache_key, ttl, content)
        
        logger.debug(f"ğŸ“¦ M3U8 å·²ç¼“å­˜: path={path}, ttl={ttl}s, size={len(content)}")
        return True
        
    except Exception as e:
        logger.error(f"è®¾ç½® m3u8 ç¼“å­˜å¤±è´¥: path={path}, error={str(e)}")
        return False


def generate_key_token(uid: str, key_path: str, expires: str, secret_key: bytes) -> str:
    """
    ä¸º key æ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„ HMAC token
    
    Args:
        uid: ç”¨æˆ· ID
        key_path: key æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        expires: è¿‡æœŸæ—¶é—´æˆ³
        secret_key: HMAC å¯†é’¥
    
    Returns:
        åå…­è¿›åˆ¶æ ¼å¼çš„ HMAC token
    """
    msg = f"{uid}:{key_path}:{expires}".encode()
    hmac_obj = hmac.new(secret_key, msg, hashlib.sha256)
    return hmac_obj.hexdigest()


def modify_m3u8_key_uri(
    m3u8_content: str,
    uid: str,
    expires: str,
    secret_key: bytes,
    m3u8_dir: str = ""
) -> str:
    """
    åŠ¨æ€ä¿®æ”¹ m3u8 æ–‡ä»¶å†…å®¹ï¼Œåœ¨ EXT-X-KEY æ ‡ç­¾çš„ URI ä¸­æ·»åŠ éªŒè¯å‚æ•°
    
    å°†åŸå§‹çš„:
    #EXT-X-KEY:METHOD=AES-128,URI="enc.key",IV=0x...
    
    ä¿®æ”¹ä¸º:
    #EXT-X-KEY:METHOD=AES-128,URI="enc.key?uid=315&expires=xxx&token=xxx",IV=0x...
    
    æ³¨æ„ï¼š
    - åªä¿®æ”¹ #EXT-X-KEY æ ‡ç­¾ä¸­çš„ URIï¼Œä¸å½±å“å…¶ä»–æ ‡ç­¾ï¼ˆå¦‚ #EXT-X-MAPï¼‰
    - ä¸ºæ¯ä¸ª key æ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„ HMAC tokenï¼ˆä½¿ç”¨ key æ–‡ä»¶è·¯å¾„è®¡ç®—ï¼‰
    
    Args:
        m3u8_content: åŸå§‹ m3u8 æ–‡ä»¶å†…å®¹
        uid: ç”¨æˆ· ID
        expires: è¿‡æœŸæ—¶é—´æˆ³
        secret_key: HMAC å¯†é’¥ï¼ˆç”¨äºç”Ÿæˆ key æ–‡ä»¶çš„ç‹¬ç«‹ tokenï¼‰
        m3u8_dir: m3u8 æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆç”¨äºè®¡ç®— key æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼‰
    
    Returns:
        ä¿®æ”¹åçš„ m3u8 å†…å®¹
    """
    if not m3u8_content:
        return m3u8_content
    
    def replace_ext_x_key_line(match):
        """
        æ›¿æ¢æ•´ä¸ª #EXT-X-KEY è¡Œä¸­çš„ URI
        match.group(0) = æ•´è¡Œ (#EXT-X-KEY:...)
        """
        line = match.group(0)
        
        # åœ¨è¡Œå†…åŒ¹é… URI å±æ€§
        def replace_uri(uri_match):
            if uri_match.group(1):  # å¸¦å¼•å·çš„æƒ…å†µ
                quote_char = uri_match.group(1)
                uri_value = uri_match.group(2)
            else:  # ä¸å¸¦å¼•å·çš„æƒ…å†µ
                quote_char = '"'
                uri_value = uri_match.group(3)
            
            # è®¡ç®— key æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ˆç”¨äºç”Ÿæˆç‹¬ç«‹ tokenï¼‰
            if uri_value.startswith('http://') or uri_value.startswith('https://'):
                # ç»å¯¹ URLï¼Œæå–è·¯å¾„éƒ¨åˆ†
                parsed = urlparse(uri_value)
                key_path = parsed.path.lstrip('/')
            elif uri_value.startswith('/'):
                # ç»å¯¹è·¯å¾„
                key_path = uri_value.lstrip('/')
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œä¸ m3u8 ç›®å½•ç»„åˆ
                if m3u8_dir:
                    key_path = os.path.join(m3u8_dir, uri_value).replace('\\', '/')
                else:
                    key_path = uri_value
            
            # ä¸ºè¿™ä¸ª key æ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„ token
            key_token = generate_key_token(uid, key_path, expires, secret_key)
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            params = urlencode({
                'uid': uid,
                'expires': expires,
                'token': key_token
            })
            
            # æ£€æŸ¥ URI æ˜¯å¦å·²ç»æœ‰æŸ¥è¯¢å‚æ•°
            if '?' in uri_value:
                new_uri = f"{uri_value}&{params}"
            else:
                new_uri = f"{uri_value}?{params}"
            
            return f'URI={quote_char}{new_uri}{quote_char}'
        
        # åŒ¹é… URI="xxx" æˆ– URI='xxx' æˆ– URI=xxx
        uri_pattern = r'URI=(["\'])([^"\']+)\1|URI=([^\s,]+)'
        modified_line = re.sub(uri_pattern, replace_uri, line)
        
        return modified_line
    
    # åªåŒ¹é… #EXT-X-KEY è¡Œï¼ˆç¡®ä¿ä¸å½±å“å…¶ä»–æ ‡ç­¾ï¼‰
    ext_x_key_pattern = r'^#EXT-X-KEY:.*$'
    modified_content = re.sub(ext_x_key_pattern, replace_ext_x_key_line, m3u8_content, flags=re.MULTILINE)
    
    return modified_content


async def check_key_access(
    key_path: str,
    uid: str,
    token: str,
    client_ip: str,
    max_uses: int,
    ttl: int,
    user_agent: Optional[str] = None
) -> Tuple[bool, Dict[str, Any]]:
    """
    æ£€æŸ¥ .key æ–‡ä»¶è®¿é—®æ˜¯å¦å…è®¸
    
    åŸºäº token æ£€æŸ¥è®¿é—®æ¬¡æ•°æ˜¯å¦è¶…è¿‡é™åˆ¶
    
    Args:
        key_path: .key æ–‡ä»¶è·¯å¾„
        uid: ç”¨æˆ· IDï¼ˆä» URL å‚æ•°è·å–ï¼‰
        token: éªŒè¯ tokenï¼ˆä» URL å‚æ•°è·å–ï¼‰
        client_ip: å®¢æˆ·ç«¯ IP
        max_uses: æœ€å¤§è®¿é—®æ¬¡æ•°
        ttl: è®¿é—®è®¡æ•°çš„ TTLï¼ˆç§’ï¼‰
        user_agent: User-Agentï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—ï¼‰
    
    Returns:
        Tuple[bool, Dict[str, Any]]: 
            - bool: True è¡¨ç¤ºå…è®¸è®¿é—®ï¼ŒFalse è¡¨ç¤ºè¢«æ‹’ç»
            - Dict: åŒ…å«è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
    """
    redis_client = redis_service.get_client()
    
    try:
        # ç”Ÿæˆè®¿é—®è®¡æ•°çš„ Redis key
        # ä½¿ç”¨ token + uid + key_path ç»„åˆï¼Œç¡®ä¿å”¯ä¸€æ€§
        access_key_content = f"{token}:{uid}:{key_path}"
        access_hash = hashlib.sha256(access_key_content.encode()).hexdigest()[:32]
        access_redis_key = f"{KEY_PROTECT_ACCESS_PREFIX}{access_hash}"
        
        # ä½¿ç”¨ INCR åŸå­æ“ä½œé€’å¢è®¡æ•°å™¨
        current_count = await redis_client.incr(access_redis_key)
        
        # é¦–æ¬¡è®¿é—®æ—¶è®¾ç½® TTL
        if current_count == 1:
            await redis_client.expire(access_redis_key, ttl)
            
            logger.info(
                f"ğŸ”‘ Key æ–‡ä»¶é¦–æ¬¡è®¿é—®: key_path={key_path}, uid={uid}, "
                f"ip={client_ip}, max_uses={max_uses}"
            )
            
            # æ­£å¸¸è®¿é—®ä¸è®°å½•æ—¥å¿—ï¼Œåªè®°å½•å¼‚å¸¸æƒ…å†µï¼ˆHMACé”™è¯¯ã€é‡æ”¾ï¼‰
            
            return True, {
                "allowed": True,
                "current_count": current_count,
                "max_uses": max_uses,
                "remaining_uses": max_uses - current_count,
                "is_first_use": True,
                "uid": uid
            }
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§ä½¿ç”¨æ¬¡æ•°
        if current_count <= max_uses:
            remaining_ttl = await redis_client.ttl(access_redis_key)
            
            logger.info(
                f"ğŸ”‘ Key æ–‡ä»¶è®¿é—®å…è®¸: key_path={key_path}, uid={uid}, "
                f"count={current_count}/{max_uses}, ip={client_ip}"
            )
            
            # æ­£å¸¸è®¿é—®ä¸è®°å½•æ—¥å¿—ï¼Œåªè®°å½•å¼‚å¸¸æƒ…å†µï¼ˆHMACé”™è¯¯ã€é‡æ”¾ï¼‰
            
            return True, {
                "allowed": True,
                "current_count": current_count,
                "max_uses": max_uses,
                "remaining_uses": max_uses - current_count,
                "is_first_use": False,
                "uid": uid,
                "remaining_ttl": remaining_ttl
            }
        else:
            # è¶…è¿‡æœ€å¤§ä½¿ç”¨æ¬¡æ•°
            remaining_ttl = await redis_client.ttl(access_redis_key)
            
            logger.warning(
                f"ğŸš« Key æ–‡ä»¶é‡æ”¾æ£€æµ‹: key_path={key_path}, uid={uid}, "
                f"count={current_count}/{max_uses}, ip={client_ip}"
            )
            
            # è®°å½•è¢«é˜»æ­¢çš„è®¿é—®
            _schedule_background_task(log_key_access(
                uid=uid,
                key_path=key_path,
                client_ip=client_ip,
                is_blocked=True,
                current_count=current_count,
                max_uses=max_uses,
                reason="max_uses_exceeded",
                user_agent=user_agent
            ))
            
            return False, {
                "allowed": False,
                "current_count": current_count,
                "max_uses": max_uses,
                "remaining_uses": 0,
                "exceeded": True,
                "uid": uid,
                "remaining_ttl": remaining_ttl,
                "reason": "Key file replay detected: maximum usage count exceeded"
            }
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥ key æ–‡ä»¶è®¿é—®å¤±è´¥: {str(e)}")
        # å‡ºé”™æ—¶é»˜è®¤å…è®¸è®¿é—®ï¼Œé¿å…å›  Redis æ•…éšœå¯¼è‡´æœåŠ¡ä¸å¯ç”¨
        return True, {
            "allowed": True,
            "error": str(e),
            "fallback": True
        }


async def log_key_access(
    uid: str,
    key_path: str,
    client_ip: str,
    is_blocked: bool,
    current_count: int = 0,
    max_uses: int = 0,
    reason: Optional[str] = None,
    user_agent: Optional[str] = None
) -> None:
    """
    è®°å½• key æ–‡ä»¶è®¿é—®äº‹ä»¶åˆ° Redis æ—¥å¿—åˆ—è¡¨
    
    Args:
        uid: ç”¨æˆ· ID
        key_path: key æ–‡ä»¶è·¯å¾„
        client_ip: å®¢æˆ·ç«¯ IP
        is_blocked: æ˜¯å¦è¢«é˜»æ­¢
        current_count: å½“å‰è®¿é—®æ¬¡æ•°
        max_uses: æœ€å¤§å…è®¸æ¬¡æ•°
        reason: é˜»æ­¢åŸå› ï¼ˆå¯é€‰ï¼‰
        user_agent: User-Agentï¼ˆå¯é€‰ï¼‰
    """
    try:
        redis_client = redis_service.get_client()
        
        # åˆ›å»ºæ—¥å¿—è®°å½•
        log_record = {
            "type": "key_access",
            "uid": uid,
            "path": key_path,
            "ip": client_ip,
            "ua": user_agent[:200] if user_agent else None,
            "count": current_count,
            "max_uses": max_uses,
            "blocked": is_blocked,
            "reason": reason,
            "timestamp": int(time.time())
        }
        
        # åºåˆ—åŒ–ä¸º JSON
        record_json = json.dumps(log_record)
        
        # ä½¿ç”¨ pipeline æ‰¹é‡æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        pipe = redis_client.pipeline()
        pipe.lpush(KEY_PROTECT_LOG_KEY, record_json)
        pipe.ltrim(KEY_PROTECT_LOG_KEY, 0, MAX_LOG_RECORDS - 1)
        pipe.expire(KEY_PROTECT_LOG_KEY, 7 * 24 * 60 * 60)  # 7å¤©è¿‡æœŸ
        await pipe.execute()
        
    except Exception as e:
        # è®°å½•æ—¥å¿—å¤±è´¥ä¸åº”è¯¥å½±å“æ­£å¸¸è¯·æ±‚
        logger.error(f"è®°å½• key æ–‡ä»¶è®¿é—®äº‹ä»¶å¤±è´¥: {str(e)}")


async def get_key_access_logs(limit: int = 300) -> List[Dict[str, Any]]:
    """
    è·å– key æ–‡ä»¶è®¿é—®æ—¥å¿—è®°å½•
    
    Args:
        limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°ï¼ˆæœ€å¤š300æ¡ï¼‰
        
    Returns:
        List of key access log records
    """
    try:
        redis_client = redis_service.get_client()
        
        # ç¡®ä¿ limit ä¸è¶…è¿‡æœ€å¤§å€¼
        limit = min(limit, MAX_LOG_RECORDS)
        
        # è·å–åˆ—è¡¨ä¸­çš„è®°å½•
        records = await redis_client.lrange(KEY_PROTECT_LOG_KEY, 0, limit - 1)
        
        # è§£æ JSON è®°å½•
        access_logs = []
        for record in records:
            try:
                access_logs.append(json.loads(record))
            except json.JSONDecodeError:
                logger.error(f"è§£æ key è®¿é—®æ—¥å¿—è®°å½•å¤±è´¥: {record}")
                continue
        
        return access_logs
        
    except Exception as e:
        logger.error(f"è·å– key è®¿é—®æ—¥å¿—å¤±è´¥: {str(e)}")
        return []


async def get_key_access_summary() -> Dict[str, Any]:
    """
    è·å– key æ–‡ä»¶è®¿é—®æ—¥å¿—æ‘˜è¦ç»Ÿè®¡
    
    Returns:
        Summary statistics
    """
    try:
        redis_client = redis_service.get_client()
        
        total_count = await redis_client.llen(KEY_PROTECT_LOG_KEY)
        
        # è·å–æœ€è¿‘çš„ä¸€äº›è®°å½•æ¥è®¡ç®—è¢«é˜»æ­¢çš„æ•°é‡
        recent_records = await redis_client.lrange(KEY_PROTECT_LOG_KEY, 0, 99)
        blocked_count = 0
        max_exceeded_count = 0
        
        for record in recent_records:
            try:
                data = json.loads(record)
                if data.get("blocked"):
                    blocked_count += 1
                    reason = data.get("reason", "")
                    if reason == "max_uses_exceeded":
                        max_exceeded_count += 1
            except json.JSONDecodeError:
                continue
        
        return {
            "total_count": total_count,
            "recent_blocked_count": blocked_count,
            "recent_max_exceeded_count": max_exceeded_count,
            "max_records": MAX_LOG_RECORDS
        }
        
    except Exception as e:
        logger.error(f"è·å– key è®¿é—®æ—¥å¿—æ‘˜è¦å¤±è´¥: {str(e)}")
        return {
            "total_count": 0,
            "recent_blocked_count": 0,
            "recent_max_exceeded_count": 0,
            "max_records": MAX_LOG_RECORDS
        }


def is_key_file(path: str, extensions: tuple) -> bool:
    """
    æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºéœ€è¦ä¿æŠ¤çš„å¯†é’¥æ–‡ä»¶
    
    Args:
        path: è¯·æ±‚è·¯å¾„
        extensions: éœ€è¦ä¿æŠ¤çš„æ‰©å±•åå…ƒç»„
    
    Returns:
        bool: æ˜¯å¦ä¸ºå¯†é’¥æ–‡ä»¶
    """
    if not path:
        return False
    
    path_lower = path.lower()
    for ext in extensions:
        if path_lower.endswith(ext.lower()):
            return True
    return False


async def get_m3u8_cache_stats() -> Dict[str, Any]:
    """
    è·å– m3u8 ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        Dict containing cache statistics:
        - cache_count: ç¼“å­˜çš„ m3u8 æ•°é‡ï¼ˆæœ€å¤šæ‰«æ100ä¸ªï¼‰
        - cache_details: æœ€å¤šæ˜¾ç¤ºå‰20ä¸ªç¼“å­˜çš„è¯¦ç»† TTL ä¿¡æ¯
        - max_displayed: è¯¦ç»†ä¿¡æ¯æ˜¾ç¤ºçš„æœ€å¤§æ•°é‡
    """
    try:
        redis_client = redis_service.get_client()
        
        # ä½¿ç”¨ SCAN è·å–æ‰€æœ‰ m3u8 ç¼“å­˜çš„ keysï¼ˆæ›´é«˜æ•ˆï¼Œä¸ä¼šé˜»å¡ Redisï¼‰
        cache_keys = []
        cursor = 0
        pattern = f"{M3U8_CONTENT_CACHE_PREFIX}*"
        max_keys = 100
        
        while True:
            cursor, keys = await redis_client.scan(cursor, match=pattern, count=100)
            # é™åˆ¶æœ€å¤šè·å– max_keys ä¸ª keys
            remaining = max_keys - len(cache_keys)
            if remaining > 0:
                cache_keys.extend(keys[:remaining])
            if cursor == 0 or len(cache_keys) >= max_keys:
                break
        
        # è·å–æ¯ä¸ªç¼“å­˜ key çš„ TTL ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰20ä¸ªï¼‰
        cache_details = []
        max_displayed = 20
        for key in cache_keys[:max_displayed]:
            try:
                ttl = await redis_client.ttl(key)
                # ä» key ä¸­æå–è·¯å¾„å“ˆå¸Œ
                key_str = key if isinstance(key, str) else key.decode('utf-8')
                path_hash = key_str.replace(M3U8_CONTENT_CACHE_PREFIX, "")
                cache_details.append({
                    "key_hash": path_hash,
                    "ttl": ttl
                })
            except Exception as ttl_error:
                logger.warning(f"è·å–ç¼“å­˜ key TTL å¤±è´¥: {key}, error: {str(ttl_error)}")
                continue
        
        return {
            "status": "ok",
            "cache_count": len(cache_keys),
            "cache_details": cache_details,
            "max_displayed": max_displayed,
            "timestamp": int(time.time())
        }
        
    except Exception as e:
        logger.error(f"è·å– m3u8 ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return {
            "status": "error",
            "error": str(e),
            "cache_count": 0,
            "cache_details": [],
            "timestamp": int(time.time())
        }
